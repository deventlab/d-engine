syntax = "proto3";
package d_engine.client;

option go_package = "github.com/deventlab/d-engine/proto/client";

import "proto/error.proto";

// Write operation-specific command
message WriteCommand {
    message Insert {
        bytes key = 1;
        bytes value = 2;
        // Time-to-live in seconds. 0 means no expiration (default).
        // Non-zero values specify expiration time in seconds from insertion.
        uint64 ttl_secs = 3;
    }
    message Delete {
        bytes key = 1;
    }
    oneof operation {
        Insert insert = 1;
        Delete delete = 2;
    }
}
message ClientWriteRequest {
    uint32 client_id = 1;
    repeated WriteCommand commands = 2;
}

// Read consistency policy for controlling read operation guarantees
//
// Allows clients to choose between performance and consistency trade-offs
// on a per-request basis when supported by the cluster configuration.
enum ReadConsistencyPolicy {

    // Lease-based reads for better performance with weaker consistency
    //
    // Leader serves reads locally without contacting followers during lease period.
    // Provides lower latency but slightly weaker consistency guarantees.
    READ_CONSISTENCY_POLICY_LEASE_READ = 0;

    // Fully linearizable reads for strongest consistency
    //
    // Leader verifies its leadership with a quorum before serving the read,
    // ensuring strict linearizability. Guarantees that all reads reflect
    // the most recent committed value in the cluster.
    READ_CONSISTENCY_POLICY_LINEARIZABLE_READ = 1;

    // Eventually consistent reads from any node
    //
    // Allows reading from any node (leader, follower, or candidate) without
    // additional consistency checks. May return stale data but provides
    // best read performance and availability. Suitable for scenarios where
    // eventual consistency is acceptable.
    READ_CONSISTENCY_POLICY_EVENTUAL_CONSISTENCY = 2;
}



message ClientReadRequest {
    uint32 client_id = 1;

    repeated bytes keys = 2; // Key list to be read

    // Optional consistency policy for this request
    //
    // When present: Client explicitly specifies consistency requirements
    // When absent: Use cluster's configured default policy
    optional ReadConsistencyPolicy consistency_policy = 3;
}

message ClientResponse {
    d_engine.error.ErrorCode error = 1;

    oneof success_result {
        bool write_ack = 2;
        ReadResults read_data = 3;
    }

    d_engine.error.ErrorMetadata metadata = 4;
}

message ClientResult {  // Renamed from ClientGetResult
    bytes key = 1;
    bytes value = 2;
  }

message ReadResults {
    repeated ClientResult results = 1;
}

// Watch event type indicating the type of change that occurred
enum WatchEventType {
    // A key was inserted or updated
    WATCH_EVENT_TYPE_PUT = 0;

    // A key was explicitly deleted
    WATCH_EVENT_TYPE_DELETE = 1;
}

// Request to watch for changes on a specific key
//
// In v1, only exact key matching is supported.
// Prefix watching may be added in future versions.
message WatchRequest {
    uint32 client_id = 1;

    // Exact key to watch for changes
    bytes key = 2;
}

// Response containing a watch event notification
message WatchResponse {
    // The key that changed
    bytes key = 1;

    // The new value (empty for DELETE events)
    bytes value = 2;

    // Type of change that occurred
    WatchEventType event_type = 3;

    // Error information if watch failed
    d_engine.error.ErrorCode error = 4;
}


service RaftClientService {
  rpc HandleClientWrite (ClientWriteRequest) returns (ClientResponse);
  rpc HandleClientRead (ClientReadRequest) returns (ClientResponse);

  // Watch for changes on a specific key
  //
  // Returns a stream of WatchResponse events whenever the watched key changes.
  // The stream remains open until the client cancels or disconnects.
  //
  // Performance characteristics:
  // - Event notification latency: typically < 100Î¼s
  // - Minimal overhead on write path (< 0.01% with 100+ watchers)
  //
  // Error handling:
  // - If the internal event buffer is full, events may be dropped
  // - Clients should use Read API to re-sync if they detect gaps
  rpc Watch (WatchRequest) returns (stream WatchResponse);
}
