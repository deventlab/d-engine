# d-engine vs etcd Benchmark Report (v0.1.2 Preview)

**Important Notice**  
⚠️ This preliminary report is based on d-engine v0.1.2 without snapshot functionality.  
✅ For final performance metrics, please wait for v0.3.0 release.

## Test Environment
**Hardware**  
Apple Mac mini (M2 Chip)  
- 8-core CPU (4 performance + 4 efficiency cores)
- 16GB Unified Memory
- All nodes and benchmarks running on single machine

**Software Versions**  
- d-engine: v0.1.2
- etcd: 3.5.x (official benchmark tool)

---

## Benchmark Methodology

### Test Configuration
| Parameter       | Value  |
|-----------------|--------|
| Key Size        | 8B     |
| Value Size      | 256B   |
| Total Requests  | 10,000 |
| Connections     | 1/10   |
| Clients         | 1/100  |

### Test Types
1. **Write Throughput**  
   - Single client vs high concurrency
2. **Read Consistency**  
   - Linearizable (strong) vs Sequential (eventual)

---

## Performance Comparison (d-engine v0.1.2 vs etcd 3.5)

**Test Configuration**  
- Key size: 8 bytes
- Value size: 256 bytes
- Total operations: 10,000
- Single node deployment (All services on Apple M2 Mac mini)

| **Test Case** | **Metric** | **d-engine** | **etcd** | **Advantage** |
| --- | --- | --- | --- | --- |
| **Basic Write** | Throughput | 385.31 ops/s | 157.85 ops/s | ✅ 2.44× d-engine |
| (1 connection, 1 client) | Avg Latency | 2,594 μs | 6,300 μs | ✅ 59% lower |
|  | p99 Latency | 4,527 μs | 16,700 μs | ✅ 73% lower |
| **High Concurrency** | Throughput | 3,972 ops/s | 5,439 ops/s | ❌ 1.37× etcd |
| (10 conns, 100 clients) | Avg Latency | 2,516 μs | 18,300 μs | ✅ 86% lower |
|  | p99 Latency | 4,359 μs | 32,400 μs | ✅ 87% lower |
| **Linear Read** | Throughput | 8,230 ops/s | 85,904 ops/s | ❌ 10.43× etcd |
| (Strong consistency) | Avg Latency | 1,212 μs | 1,100 μs | ❌ 10% higher |
|  | p99 Latency | 1,467 μs | 3,200 μs | ✅ 54% lower |
| **Sequential Read** | Throughput | 40,860 ops/s | 124,631 ops/s | ❌ 3.05× etcd |
| (Eventual consistency) | Avg Latency | 243 μs | 700 μs | ✅ 65% lower |
|  | p99 Latency | 529 μs | 2,800 μs | ✅ 81% lower |

**Important Notes**  
1. d-engine architecture uses single-threaded event-driven design
2. Tested on d-engine v0.1.2 (without snapshot functionality)
3. etcd 3.5 benchmark using official tools
4. All services co-located on same hardware (M2/16GB)

---

## Test Details

### d-engine tests
```bash
# Write Performance Test, Single Client (PUT Operation)
./target/release/d-engine-bench  \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 1 --clients 1 --sequential-keys --total 10000 --key-size 8 --value-size 256 \
    put

# Write Performance Test, High Concurrency (PUT Operation)
./target/release/d-engine-bench  \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 10 --clients 100 --sequential-keys --total 10000 --key-size 8 --value-size 256 \
    put

# Linearizable Read Performance Test
./target/release/d-engine-bench \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 10 --clients 100 --sequential-keys --total 10000 --key-size 8 \
    range --consistency l

# Serializable Read Performance Test
./target/release/d-engine-bench \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 10 --clients 100 --sequential-keys --total 10000 --key-size 8 \
    range --consistency s
```

### etcd tests
```bash
# Write Performance Test, Single Client (PUT Operation)
export ENDPOINTS=http://0.0.0.0:2380,http://0.0.0.0:2381,http://0.0.0.0:2382
benchmark \
  --endpoints=${ENDPOINTS} --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256

# Write Performance Test, High Concurrency (PUT Operation)
benchmark \
  --endpoints=${ENDPOINTS} --target-leader --conns=10 --clients=100 put --key-size=8 --sequential-keys --total=10000 --val-size=256

# Linearizable Read Performance Test
benchmark \
  --endpoints=${ENDPOINTS} --conns=10 --clients=100 range key_ --consistency=l --total=10000 

# Serializable Read Performance Test
benchmark \
  --endpoints=${ENDPOINTS} --conns=10 --clients=100 range key_ --consistency=s --total=10000
```

---
## Test Setup Guides

### d-engine Cluster Setup

```bash
# Start 3-node cluster using Makefile
cd /examples/three-nodes-cluster/
make start-cluster
```

### etcd Cluster Setup

1. **Start etcd nodes** (3 terminals):

```bash
#!/bin/bash

# All nodes share the same configuration.
export CLUSTER_TOKEN="etcd-cluster"
export INITIAL_CLUSTER_STATE="new"
export INITIAL_CLUSTER="infra0=http://0.0.0.0:2380,infra1=http://0.0.0.0:2381,infra2=http://0.0.0.0:2382"

# Define node configuration array: node name client port peer port
nodes=(
  "infra0 12379 2380"
  "infra1 22379 2381" 
  "infra2 32379 2382"
)

for node in "${nodes[@]}"; do
  IFS=' ' read -r name client_port peer_port <<< "$node"
  
  etcd --name "$name" \
    --listen-client-urls "http://0.0.0.0:${client_port}" \
    --advertise-client-urls "http://0.0.0.0:${client_port}" \
    --listen-peer-urls "http://0.0.0.0:${peer_port}" \
    --initial-cluster-token "$CLUSTER_TOKEN" \
    --initial-cluster "$INITIAL_CLUSTER" \
    --initial-cluster-state "$INITIAL_CLUSTER_STATE" &
done

echo "etcd cluster started with 3 nodes"
echo "validate cluster status:"
etcdctl --endpoints=localhost:12379,localhost:22379,localhost:32379 endpoint status --write-out=table

```

2. **Install Benchmark Tool**:

```bash
go install go.etcd.io/etcd/v3/tools/benchmark@v3.5.0
```


---

## Limitations & Next Steps

1. **Current Limitations**
    - Single-node testing environment
    - Missing snapshot functionality in d-engine
    - Preliminary load patterns tested
2. **Roadmap**
    - v0.2.0 with complete feature set
    - Distributed environment testing
    - Long-running stability tests
    - Mixed workload analysis

---

## Conclusion (Preview)

While etcd currently demonstrates higher raw throughput, d-engine v0.1.2 shows promising latency characteristics particularly for write-intensive workloads. The final evaluation should be reserved for v0.2.0 with complete feature parity.

**Recommendation**

Consider d-engine for latency-sensitive applications requiring strong consistency, while etcd remains better suited for high-throughput read scenarios in its current state.
