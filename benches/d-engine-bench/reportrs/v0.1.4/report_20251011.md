# **d-engine vs etcd Benchmark Report (v0.1.3)**

**Important Notice**

⚠️ This report is based on **d-engine v0.1.3**, with additional testing under different snapshot and cluster configurations.

✅ Snapshot functionality is now available but optional, and performance varies depending on cluster size and persistence strategy.

---

## **Test Environment**

**Hardware**

Apple Mac mini (M2 Chip)

- 8-core CPU (4 performance + 4 efficiency cores)
- 16GB Unified Memory
- All nodes and benchmarks running on a single machine

**Software Versions**

- d-engine: v0.1.3
- etcd: 3.5.x (official benchmark tool)

---

## **Benchmark Methodology**

### **Test Configuration**

| **Parameter**  | **Value** |
| -------------- | --------- |
| Key Size       | 8B        |
| Value Size     | 256B      |
| Total Requests | 10,000    |
| Connections    | 1/10      |
| Clients        | 1/100     |

**Persistence & Snapshot Settings**

- **MemFirst + Batch Flush** (threshold=1000, interval=100ms)
- Snapshot tested **on** and **off**
- Snapshot parameters:

```
[raft.snapshot]
enable = true
max_log_entries_before_snapshot = 100000
snapshot_cool_down_since_last_check = { secs = 10 }
```

---

## Comparison: d-engine v0.1.3 vs v0.1.4

| **Test Case**                      | **Metric** | **v0.1.3**               | **v0.1.4**                           | **Change** | **Notes**                                      |
| ---------------------------------- | ---------- | ------------------------ | ------------------------------------ | ---------- | ---------------------------------------------- |
| **Write – 1 conn / 1 client**      | Throughput | 358.28 ops/s             | **451.41 ops/s**                     | ⬆︎ +26%   | Minor improvement from WAL tuning              |
| Avg Latency                        | 2,789 μs   | **2,214 μs**             | ⬇︎ -21%                             |            |                                                |
| **Write – 10 conns / 100 clients** | Throughput | 4,088.54 ops/s           | **5,112.83 ops/s**                   | ⬆︎ +25%   |                                                |
| Avg Latency                        | 2,443 μs   | **1,953 μs**             | ⬇︎ -20%                             |            |                                                |
| **Linearizable Read (L)**          | Throughput | 8,294.53 ops/s           | **7,399.24 ops/s**                   | ⬇︎ -10%   | Slight overhead from read policy layer         |
| Avg Latency                        | 1,203 μs   | **27,002 μs**            | ⬆︎ due to full quorum reads         |            |                                                |
| **Lease-based Read (S)**           | Throughput | 39,938.10 ops/s          | **63,711.84 ops/s**                  | ⬆︎ +59%   | Strong consistency preserved under valid lease |
| Avg Latency                        | 249 μs     | **3,135 μs**             | Expected increase due to lease check |            |                                                |
| **Eventual Read (E)**              | Throughput | 39,938.10 ops/s (S-mode) | **72,388.99 ops/s**                  | ⬆︎ +81%   | Maximum performance, relaxed consistency       |

---

## Performance Comparison (d-engine v0.1.4 vs etcd 3.5)

**Test Configuration**

- Key size: 8 bytes
- Value size: 256 bytes
- Total operations: 10,000
- Single machine deployment (Apple M2 Mac mini, all services co-located)

| **Test Case**            | **Metric**  | **rocksdb (0.1.3)** | **rocksdb (0.1.4)** | **etcd 3.5**  | **Advantage**                        |
| ------------------------ | ----------- | ------------------- | ------------------- | ------------- | ------------------------------------ |
| **Basic Write**          | Throughput  | 456.28 ops/s        | 451.41 ops/s        | 157.85 ops/s  | ✅ ~2.86× RocksDB vs etcd            |
| (1 connection, 1 client) | Avg Latency | 2,190 μs            | 2,215 μs            | 6,300 μs      | ✅ ~65% lower RocksDB vs etcd        |
|                          | p99 Latency | 5,023 μs            | 5,807 μs            | 16,700 μs     | ✅ ~65% lower RocksDB vs etcd        |
| **High Concurrency**     | Throughput  | 4,869.72 ops/s      | 5,112.83 ops/s      | 5,439 ops/s   | ⚠ Slightly lower than etcd          |
| (10 conns, 100 clients)  | Avg Latency | 2,050 μs            | 1,953 μs            | 18,300 μs     | ✅ ~89% lower RocksDB vs etcd        |
|                          | p99 Latency | 4,143 μs            | 3,187 μs            | 32,400 μs     | ✅ ~90% lower RocksDB vs etcd        |
| **Linear Read**          | Throughput  | 10,423.33 ops/s     | 7,399.24 ops/s      | 85,904 ops/s  | ❌ 11.6× slower vs etcd              |
| (Strong consistency)     | Avg Latency | 956 μs              | 27,002 μs           | 1,100 μs      | ❌ higher due to full quorum         |
|                          | p99 Latency | 1,166 μs            | 50,271 μs           | 3,200 μs      | ❌ much higher tail latency          |
| **Lease-based Read**     | Throughput  | –                   | 63,711.84 ops/s     | –             | ✅ 8–10× faster than linear          |
| (Strong consistency)     | Avg Latency | –                   | 3,135 μs            | –             | ✅ reasonable for strong reads       |
|                          | p99 Latency | –                   | 7,631 μs            | –             | ✅ improved tail performance         |
| **Sequential Read**      | Throughput  | 44,927.66 ops/s     | 72,388.99 ops/s     | 124,631 ops/s | ✅ 58% faster than v0.1.3            |
| (Eventual consistency)   | Avg Latency | 221 μs              | 2,757 μs            | 700 μs        | ⚠ higher due to scale & concurrency |
|                          | p99 Latency | 462 μs              | 22,159 μs           | 2,800 μs      | ⚠ significantly higher tail         |

**Important Notes**

1. d-engine architecture uses single-threaded, event-driven design
2. Tested on **d-engine v0.1.3 (3-node, snapshot OFF, MemFirst+Batch Flush)**
3. etcd 3.5 benchmark uses official tools and default configuration
4. All services co-located on the same Apple M2 (16GB) machine

---

## **Key Observations**

1. **Snapshot ON vs OFF** (3-node):
   - Minimal impact on single-node throughput.
   - Linearizable reads show better throughput with snapshot enabled.
2. **Scaling to 5 Nodes**:
   - Throughput decreases due to Raft quorum overhead and learners.
   - Sequential reads impacted by learners rejecting client requests.
3. **Latency Trends**:
   - Even under higher concurrency, average latency remains under ~2.6 ms for writes and ~1.9 ms for strong reads.
   - p99 latency grows with cluster size.

---

## **Limitations & Next Steps**

1. **Known Limitations**
   - All tests run on single Apple M2 machine.
   - 5-node tests exposed learner read issues.
   - etcd comparison not yet repeated for v0.1.3.
2. **Next Steps**
   - Run distributed multi-machine benchmarks.
   - Evaluate long-running workloads with snapshots and log compaction.
   - Re-run etcd comparison for v0.1.3 parity.

---

## **Conclusion (v0.1.3)**

- **d-engine v0.1.3** introduces snapshot support without noticeable overhead in 3-node clusters.
- Write performance slightly improved at high concurrency.
- Scaling to 5 nodes introduces quorum cost and learner read issues.
- Latency remains competitive, suitable for latency-sensitive workloads.

---

## Test Details

### d-engine tests

```bash
# Write Performance Test, Single Client (PUT Operation)
./target/release/d-engine-bench  \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 1 --clients 1 --sequential-keys --total 10000 --key-size 8 --value-size 256 \
    put

# Write Performance Test, High Concurrency (PUT Operation)
./target/release/d-engine-bench  \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 10 --clients 100 --sequential-keys --total 10000 --key-size 8 --value-size 256 \
    put

# Strong consistency (linearizable)
./target/release/d-engine-bench \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 200 --clients 1000 --sequential-keys --total 100000 --key-size 8 \
    range --consistency l

# Lease-based reads (better performance with still strong consistency)
./target/release/d-engine-bench \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 200 --clients 1000 --sequential-keys --total 100000 --key-size 8 \
    range --consistency s

# Eventual consistency (highest performance, may return stale data)
./target/release/d-engine-bench \
    --endpoints http://127.0.0.1:9081 --endpoints http://127.0.0.1:9082 --endpoints http://127.0.0.1:9083 \
    --conns 200 --clients 1000 --sequential-keys --total 100000 --key-size 8 \
    range --consistency e
```

### etcd tests

```bash
# Write Performance Test, Single Client (PUT Operation)
export ENDPOINTS=http://0.0.0.0:2380,http://0.0.0.0:2381,http://0.0.0.0:2382
benchmark \
  --endpoints=${ENDPOINTS} --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256

# Write Performance Test, High Concurrency (PUT Operation)
benchmark \
  --endpoints=${ENDPOINTS} --target-leader --conns=10 --clients=100 put --key-size=8 --sequential-keys --total=10000 --val-size=256

# Linearizable Read Performance Test
benchmark \
  --endpoints=${ENDPOINTS} --conns=10 --clients=100 range key_ --consistency=l --total=10000

# Serializable Read Performance Test
benchmark \
  --endpoints=${ENDPOINTS} --conns=10 --clients=100 range key_ --consistency=s --total=10000
```

---
